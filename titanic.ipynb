{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFtIpR0dE1TW78C7x9Rcyd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lavasany/DeepLearning/blob/main/titanic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UhgggLzAcyZ8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "sns.set(style=\"darkgrid\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "import string\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "SEED = 42"
      ],
      "metadata": {
        "id": "SK9xF4NZdEd0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in the training and test sets\n",
        "df_train = pd.read_csv('/train.csv')\n",
        "# print(len(train_df))\n",
        "df_test = pd.read_csv('/test.csv')\n",
        "# print(len(test_df))\n",
        "result_df = pd.read_csv('/gender_submission.csv')   # 100% result\n",
        "# print(len(result_df))"
      ],
      "metadata": {
        "id": "Tl0KZJSYdJ6K"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def concat_df(train_data, test_data):\n",
        "    # Returns a concatenated df of training and test set\n",
        "    return pd.concat([train_data, test_data], sort=True).reset_index(drop=True)\n",
        "\n",
        "def divide_df(all_data):\n",
        "    # Returns divided dfs of training and test set\n",
        "    return all_data.loc[:890], all_data.loc[891:].drop(['Survived'], axis=1)\n",
        "\n",
        "\n",
        "df_all = concat_df(df_train, df_test)\n",
        "\n",
        "df_train.name = 'Training Set'\n",
        "df_test.name = 'Test Set'\n",
        "df_all.name = 'All Set'\n",
        "\n",
        "dfs = [df_train, df_test]\n"
      ],
      "metadata": {
        "id": "cfjsKsydeImR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of Training Examples = {}'.format(df_train.shape[0]))\n",
        "print('Number of Test Examples = {}\\n'.format(df_test.shape[0]))\n",
        "print('Training X Shape = {}'.format(df_train.shape))\n",
        "print('Training y Shape = {}\\n'.format(df_train['Survived'].shape[0]))\n",
        "print('Test X Shape = {}'.format(df_test.shape))\n",
        "print('Test y Shape = {}\\n'.format(df_test.shape[0]))\n",
        "print(df_train.columns)\n",
        "print(df_test.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaeO_8t8eTy_",
        "outputId": "e4dbb831-c38c-4f2c-9115-5e3acf5a5a52"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Training Examples = 891\n",
            "Number of Test Examples = 418\n",
            "\n",
            "Training X Shape = (891, 12)\n",
            "Training y Shape = 891\n",
            "\n",
            "Test X Shape = (418, 11)\n",
            "Test y Shape = 418\n",
            "\n",
            "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
            "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
            "      dtype='object')\n",
            "Index(['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch',\n",
            "       'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################################### Missing Values #############################################################\n",
        "def display_missing(df):\n",
        "    for col in df.columns.tolist():\n",
        "        print('{} column missing values: {}'.format(col, df[col].isnull().sum()))\n",
        "    print('\\n')\n",
        "\n",
        "\n",
        "for df in dfs:\n",
        "    print('{}'.format(df.name))\n",
        "    display_missing(df)\n",
        "\n",
        "#  from below, Pclass and Sex groups have distinct median Age values\n",
        "df_all_corr = df_all.corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\n",
        "df_all_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\n",
        "age_by_pclass_sex = df_all.groupby(['Sex', 'Pclass']).median()['Age']\n",
        "\n",
        "for pclass in range(1, 4):\n",
        "    for sex in ['female', 'male']:\n",
        "        print('Median age of Pclass {} {}s: {}'.format(pclass, sex, age_by_pclass_sex[sex][pclass]))\n",
        "print('Median age of all passengers: {}'.format(df_all['Age'].median()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKwkJb1_e6Hh",
        "outputId": "17ca705f-18be-4fb0-e47a-b4c55172b563"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set\n",
            "PassengerId column missing values: 0\n",
            "Survived column missing values: 0\n",
            "Pclass column missing values: 0\n",
            "Name column missing values: 0\n",
            "Sex column missing values: 0\n",
            "Age column missing values: 177\n",
            "SibSp column missing values: 0\n",
            "Parch column missing values: 0\n",
            "Ticket column missing values: 0\n",
            "Fare column missing values: 0\n",
            "Cabin column missing values: 687\n",
            "Embarked column missing values: 2\n",
            "\n",
            "\n",
            "Test Set\n",
            "PassengerId column missing values: 0\n",
            "Pclass column missing values: 0\n",
            "Name column missing values: 0\n",
            "Sex column missing values: 0\n",
            "Age column missing values: 86\n",
            "SibSp column missing values: 0\n",
            "Parch column missing values: 0\n",
            "Ticket column missing values: 0\n",
            "Fare column missing values: 1\n",
            "Cabin column missing values: 327\n",
            "Embarked column missing values: 0\n",
            "\n",
            "\n",
            "Median age of Pclass 1 females: 36.0\n",
            "Median age of Pclass 1 males: 42.0\n",
            "Median age of Pclass 2 females: 28.0\n",
            "Median age of Pclass 2 males: 29.5\n",
            "Median age of Pclass 3 females: 22.0\n",
            "Median age of Pclass 3 males: 25.0\n",
            "Median age of all passengers: 28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filling the missing values in Age with the medians of Sex and Pclass groups\n",
        "df_all['Age'] = df_all.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))\n",
        "# Filling the missing values in Embarked with S\n",
        "df_all['Embarked'] = df_all['Embarked'].fillna('S')\n",
        "\n",
        "med_fare = df_all.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()[3][0][0]"
      ],
      "metadata": {
        "id": "5a5RonxOfVZP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filling the missing value in Fare with the median Fare of 3rd class alone passenger\n",
        "df_all['Fare'] = df_all['Fare'].fillna(med_fare)\n"
      ],
      "metadata": {
        "id": "lMMyPZakfo4r"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Deck column from the first letter of the Cabin column (M stands for Missing)\n",
        "df_all['Deck'] = df_all['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\n",
        "\n",
        "df_all_decks = df_all.groupby(['Deck', 'Pclass']).count().drop(columns=['Survived', 'Sex', 'Age', 'SibSp', 'Parch',\n",
        "                                                                        'Fare', 'Embarked', 'Cabin', 'PassengerId',\n",
        "                                                                        'Ticket']).rename(columns={'Name': 'Count'}).transpose()"
      ],
      "metadata": {
        "id": "DTPBt3CTgGhy"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pclass_dist(df):\n",
        "\n",
        "# Creating a dictionary for every passenger class count in every deck\n",
        "  deck_counts = {'A': {}, 'B': {}, 'C': {}, 'D': {}, 'E': {}, 'F': {}, 'G': {}, 'M': {}, 'T': {}}\n",
        "  decks = df.columns.levels[0]\n",
        "\n",
        "  for deck in decks:\n",
        "     for pclass in range(1, 4):\n",
        "             try:\n",
        "                count = df[deck][pclass][0]\n",
        "                deck_counts[deck][pclass] = count\n",
        "             except KeyError:\n",
        "                deck_counts[deck][pclass] = 0\n",
        "\n",
        "  df_decks = pd.DataFrame(deck_counts)\n",
        "  deck_percentages = {}\n",
        "  for col in df_decks.columns:\n",
        "      deck_percentages[col] = [(count / df_decks[col].sum()) * 100\n",
        "for count in df_decks[col]]\n",
        "  return deck_counts, deck_percentages"
      ],
      "metadata": {
        "id": "NEHwoQQPgLTx"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Passenger in the T deck is changed to A\n",
        "idx = df_all[df_all['Deck'] == 'T'].index\n",
        "df_all.loc[idx, 'Deck'] = 'A'\n",
        "\n",
        "df_all_decks_survived = df_all.groupby(['Deck', 'Survived']).count().drop(\n",
        "    columns=['Sex', 'Age', 'SibSp', 'Parch', 'Fare',\n",
        "             'Embarked', 'Pclass', 'Cabin', 'PassengerId', 'Ticket']).rename(columns={'Name': 'Count'}).transpose()"
      ],
      "metadata": {
        "id": "Jsbu4jlR-YUR"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_survived_dist(df):\n",
        "    # Creating a dictionary for every survival count in every deck\n",
        "    surv_counts = {'A': {}, 'B': {}, 'C': {}, 'D': {}, 'E': {}, 'F': {}, 'G': {}, 'M': {}}\n",
        "    decks = df.columns.levels[0]\n",
        "\n",
        "    for deck in decks:\n",
        "        for survive in range(0, 2):\n",
        "            surv_counts[deck][survive] = df[deck][survive][0]\n",
        "\n",
        "    df_surv = pd.DataFrame(surv_counts)\n",
        "    surv_percentages = {}\n",
        "\n",
        "    for col in df_surv.columns:\n",
        "        surv_percentages[col] = [(count / df_surv[col].sum()) * 100 for count in df_surv[col]]\n",
        "\n",
        "    return surv_counts, surv_percentages\n",
        "\n",
        "all_surv_count, all_surv_per = get_survived_dist(df_all_decks_survived)\n",
        "\n",
        "df_all['Deck'] = df_all['Deck'].replace(['A', 'B', 'C'], 'ABC')\n",
        "df_all['Deck'] = df_all['Deck'].replace(['D', 'E'], 'DE')\n",
        "df_all['Deck'] = df_all['Deck'].replace(['F', 'G'], 'FG')\n",
        "\n",
        "print(df_all['Deck'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Swixe7Xd-fRv",
        "outputId": "501abd30-3df0-4078-9130-cc4b5f98ea2e"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "M      1014\n",
            "ABC     182\n",
            "DE       87\n",
            "FG       26\n",
            "Name: Deck, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping the Cabin feature\n",
        "df_all.drop(['Cabin'], inplace=True, axis=1)\n",
        "\n",
        "df_train, df_test = divide_df(df_all)\n",
        "dfs = [df_train, df_test]\n",
        "\n",
        "for df in dfs:\n",
        "    display_missing(df)\n",
        "\n",
        "survived = df_train['Survived'].value_counts()[1]\n",
        "not_survived = df_train['Survived'].value_counts()[0]\n",
        "survived_per = survived / df_train.shape[0] * 100\n",
        "not_survived_per = not_survived / df_train.shape[0] * 100\n",
        "\n",
        "print('{} of {} passengers survived and it is the {:.2f}% of the training set.'.format(survived, df_train.shape[0], survived_per))\n",
        "print('{} of {} passengers didnt survive and it is the {:.2f}% of the training set.'.format(not_survived, df_train.shape[0], not_survived_per))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNaViyZQ-9yD",
        "outputId": "03f3643f-2b77-4e11-d6ef-06fe22c251f7"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Age column missing values: 0\n",
            "Embarked column missing values: 0\n",
            "Fare column missing values: 0\n",
            "Name column missing values: 0\n",
            "Parch column missing values: 0\n",
            "PassengerId column missing values: 0\n",
            "Pclass column missing values: 0\n",
            "Sex column missing values: 0\n",
            "SibSp column missing values: 0\n",
            "Survived column missing values: 0\n",
            "Ticket column missing values: 0\n",
            "Deck column missing values: 0\n",
            "\n",
            "\n",
            "Age column missing values: 0\n",
            "Embarked column missing values: 0\n",
            "Fare column missing values: 0\n",
            "Name column missing values: 0\n",
            "Parch column missing values: 0\n",
            "PassengerId column missing values: 0\n",
            "Pclass column missing values: 0\n",
            "Sex column missing values: 0\n",
            "SibSp column missing values: 0\n",
            "Ticket column missing values: 0\n",
            "Deck column missing values: 0\n",
            "\n",
            "\n",
            "342 of 891 passengers survived and it is the 38.38% of the training set.\n",
            "549 of 891 passengers didnt survive and it is the 61.62% of the training set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################################## Categorical Features ################################################\n",
        "cat_features = ['Embarked', 'Parch', 'Pclass', 'Sex', 'SibSp', 'Deck']\n",
        "\n",
        "################################################## Feature Engineering #################################################\n",
        "df_all['Fare'] = pd.qcut(df_all['Fare'], 13)\n",
        "df_all['Age'] = pd.qcut(df_all['Age'], 10)\n",
        "df_all['Family_Size'] = df_all['SibSp'] + df_all['Parch'] + 1\n",
        "family_map = {1: 'Alone', 2: 'Small', 3: 'Small', 4: 'Small', 5: 'Medium', 6: 'Medium', 7: 'Large', 8: 'Large', 11: 'Large'}\n",
        "df_all['Family_Size_Grouped'] = df_all['Family_Size'].map(family_map)\n",
        "\n",
        "\n",
        "df_all['Ticket_Frequency'] = df_all.groupby('Ticket')['Ticket'].transform('count')\n",
        "\n",
        "df_all['Title'] = df_all['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\n",
        "df_all['Is_Married'] = 0\n",
        "df_all['Is_Married'].loc[df_all['Title'] == 'Mrs'] = 1\n",
        "df_all['Title'] = df_all['Title'].replace(['Miss', 'Mrs','Ms', 'Mlle', 'Lady', 'Mme', 'the Countess', 'Dona'], 'Miss/Mrs/Ms')\n",
        "df_all['Title'] = df_all['Title'].replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev'], 'Dr/Military/Noble/Clergy')"
      ],
      "metadata": {
        "id": "oFFVat_U_Grb"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_surname(data):\n",
        "    families = []\n",
        "\n",
        "    for i in range(len(data)):\n",
        "        name = data.iloc[i]\n",
        "\n",
        "        if '(' in name:\n",
        "            name_no_bracket = name.split('(')[0]\n",
        "        else:\n",
        "            name_no_bracket = name\n",
        "\n",
        "        family = name_no_bracket.split(',')[0]\n",
        "        title = name_no_bracket.split(',')[1].strip().split(' ')[0]\n",
        "\n",
        "        for c in string.punctuation:\n",
        "            family = family.replace(c, '').strip()\n",
        "\n",
        "        families.append(family)\n",
        "\n",
        "    return families"
      ],
      "metadata": {
        "id": "HmKUFh62_Tph"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all['Family'] = extract_surname(df_all['Name'])\n",
        "df_train = df_all.loc[:890]\n",
        "df_test = df_all.loc[891:]\n",
        "dfs = [df_train, df_test]\n",
        "\n",
        "# Creating a list of families and tickets that are occuring in both training and test set\n",
        "non_unique_families = [x for x in df_train['Family'].unique() if x in df_test['Family'].unique()]\n",
        "non_unique_tickets = [x for x in df_train['Ticket'].unique() if x in df_test['Ticket'].unique()]\n",
        "\n",
        "df_family_survival_rate = df_train.groupby('Family')['Survived', 'Family','Family_Size'].median()\n",
        "df_ticket_survival_rate = df_train.groupby('Ticket')['Survived', 'Ticket','Ticket_Frequency'].median()\n",
        "\n",
        "family_rates = {}\n",
        "ticket_rates = {}\n",
        "\n",
        "for i in range(len(df_family_survival_rate)):\n",
        "    # Checking a family exists in both training and test set, and has members more than 1\n",
        "    if df_family_survival_rate.index[i] in non_unique_families and df_family_survival_rate.iloc[i, 1] > 1:\n",
        "        family_rates[df_family_survival_rate.index[i]] = df_family_survival_rate.iloc[i, 0]\n",
        "\n",
        "for i in range(len(df_ticket_survival_rate)):\n",
        "    # Checking a ticket exists in both training and test set, and has members more than 1\n",
        "    if df_ticket_survival_rate.index[i] in non_unique_tickets and df_ticket_survival_rate.iloc[i, 1] > 1:\n",
        "        ticket_rates[df_ticket_survival_rate.index[i]] = df_ticket_survival_rate.iloc[i, 0]"
      ],
      "metadata": {
        "id": "fhQ_BvaO_bPW"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###################################\n",
        "mean_survival_rate = np.mean(df_train['Survived'])\n",
        "\n",
        "train_family_survival_rate = []\n",
        "train_family_survival_rate_NA = []\n",
        "test_family_survival_rate = []\n",
        "test_family_survival_rate_NA = []\n",
        "\n",
        "for i in range(len(df_train)):\n",
        "    if df_train['Family'][i] in family_rates:\n",
        "        train_family_survival_rate.append(family_rates[df_train['Family'][i]])\n",
        "        train_family_survival_rate_NA.append(1)\n",
        "    else:\n",
        "        train_family_survival_rate.append(mean_survival_rate)\n",
        "        train_family_survival_rate_NA.append(0)\n",
        "\n",
        "for i in range(len(df_test)):\n",
        "    if df_test['Family'].iloc[i] in family_rates:\n",
        "        test_family_survival_rate.append(family_rates[df_test['Family'].iloc[i]])\n",
        "        test_family_survival_rate_NA.append(1)\n",
        "    else:\n",
        "        test_family_survival_rate.append(mean_survival_rate)\n",
        "        test_family_survival_rate_NA.append(0)\n",
        "\n",
        "df_train['Family_Survival_Rate'] = train_family_survival_rate\n",
        "df_train['Family_Survival_Rate_NA'] = train_family_survival_rate_NA\n",
        "df_test['Family_Survival_Rate'] = test_family_survival_rate\n",
        "df_test['Family_Survival_Rate_NA'] = test_family_survival_rate_NA\n",
        "\n",
        "train_ticket_survival_rate = []\n",
        "train_ticket_survival_rate_NA = []\n",
        "test_ticket_survival_rate = []\n",
        "test_ticket_survival_rate_NA = []\n",
        "\n",
        "for i in range(len(df_train)):\n",
        "    if df_train['Ticket'][i] in ticket_rates:\n",
        "        train_ticket_survival_rate.append(ticket_rates[df_train['Ticket'][i]])\n",
        "        train_ticket_survival_rate_NA.append(1)\n",
        "    else:\n",
        "        train_ticket_survival_rate.append(mean_survival_rate)\n",
        "        train_ticket_survival_rate_NA.append(0)\n",
        "\n",
        "for i in range(len(df_test)):\n",
        "    if df_test['Ticket'].iloc[i] in ticket_rates:\n",
        "        test_ticket_survival_rate.append(ticket_rates[df_test['Ticket'].iloc[i]])\n",
        "        test_ticket_survival_rate_NA.append(1)\n",
        "    else:\n",
        "        test_ticket_survival_rate.append(mean_survival_rate)\n",
        "        test_ticket_survival_rate_NA.append(0)\n",
        "\n",
        "df_train['Ticket_Survival_Rate'] = train_ticket_survival_rate\n",
        "df_train['Ticket_Survival_Rate_NA'] = train_ticket_survival_rate_NA\n",
        "df_test['Ticket_Survival_Rate'] = test_ticket_survival_rate\n",
        "df_test['Ticket_Survival_Rate_NA'] = test_ticket_survival_rate_NA\n",
        "\n",
        "for df in [df_train, df_test]:\n",
        "    df['Survival_Rate'] = (df['Ticket_Survival_Rate'] + df['Family_Survival_Rate']) / 2\n",
        "    df['Survival_Rate_NA'] = (df['Ticket_Survival_Rate_NA'] + df['Family_Survival_Rate_NA']) / 2\n"
      ],
      "metadata": {
        "id": "1Ps1vLOe_o7u"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################################## Label Encoding Non-Numerical Features #########################################\n",
        "non_numeric_features = ['Embarked', 'Sex', 'Deck', 'Title', 'Family_Size_Grouped', 'Age', 'Fare']\n",
        "\n",
        "for df in dfs:\n",
        "    for feature in non_numeric_features:\n",
        "        df[feature] = LabelEncoder().fit_transform(df[feature])"
      ],
      "metadata": {
        "id": "oNFHqIob_2XT"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################################### One-Hot Encoding the Categorical Features ####################################\n",
        "cat_features = ['Pclass', 'Sex', 'Deck', 'Embarked', 'Title', 'Family_Size_Grouped']\n",
        "encoded_features = []\n",
        "\n",
        "for df in dfs:\n",
        "    for feature in cat_features:\n",
        "        encoded_feat = OneHotEncoder().fit_transform(df[feature].values.reshape(-1, 1)).toarray()\n",
        "        n = df[feature].nunique()\n",
        "        cols = ['{}_{}'.format(feature, n) for n in range(1, n + 1)]\n",
        "        encoded_df = pd.DataFrame(encoded_feat, columns=cols)\n",
        "        encoded_df.index = df.index\n",
        "        encoded_features.append(encoded_df)\n",
        "\n",
        "df_train = pd.concat([df_train, *encoded_features[:6]], axis=1)\n",
        "df_test = pd.concat([df_test, *encoded_features[6:]], axis=1)"
      ],
      "metadata": {
        "id": "FkUxgUk2_-6d"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################################## Conclusion ####################################################################\n",
        "df_all = concat_df(df_train, df_test)\n",
        "drop_cols = ['Deck', 'Embarked', 'Family', 'Family_Size', 'Family_Size_Grouped', 'Survived',\n",
        "             'Name', 'Parch', 'PassengerId', 'Pclass', 'Sex', 'SibSp', 'Ticket', 'Title',\n",
        "            'Ticket_Survival_Rate', 'Family_Survival_Rate', 'Ticket_Survival_Rate_NA', 'Family_Survival_Rate_NA']\n",
        "\n",
        "df_all.drop(columns=drop_cols, inplace=True)"
      ],
      "metadata": {
        "id": "fuESM8u5AVIh"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################# Model ################################################################\n",
        "X_train = StandardScaler().fit_transform(df_train.drop(columns=drop_cols))\n",
        "y_train = df_train['Survived'].values\n",
        "X_test = StandardScaler().fit_transform(df_test.drop(columns=drop_cols))\n",
        "\n",
        "print('X_train shape: {}'.format(X_train.shape))\n",
        "print('y_train shape: {}'.format(y_train.shape))\n",
        "print('X_test shape: {}'.format(X_test.shape))\n",
        "\n",
        "\n",
        "single_best_model = RandomForestClassifier(criterion='gini',\n",
        "                                           n_estimators=1100,\n",
        "                                           max_depth=5,\n",
        "                                           min_samples_split=4,\n",
        "                                           min_samples_leaf=5,\n",
        "                                           max_features='auto',\n",
        "                                           oob_score=True,\n",
        "                                           random_state=SEED,\n",
        "                                           n_jobs=-1,\n",
        "                                           verbose=1)\n",
        "\n",
        "leaderboard_model = RandomForestClassifier(criterion='gini',\n",
        "                                           n_estimators=1750,\n",
        "                                           max_depth=7,\n",
        "                                           min_samples_split=6,\n",
        "                                           min_samples_leaf=6,\n",
        "                                           max_features='auto',\n",
        "                                           oob_score=True,\n",
        "                                           random_state=SEED,\n",
        "                                           n_jobs=-1,\n",
        "                                           verbose=1)\n",
        "\n",
        "N = 5\n",
        "oob = 0\n",
        "probs = pd.DataFrame(np.zeros((len(X_test), N * 2)),\n",
        "                     columns=['Fold_{}_Prob_{}'.format(i, j) for i in range(1, N + 1) for j in range(2)])\n",
        "importances = pd.DataFrame(np.zeros((X_train.shape[1], N)), columns=['Fold_{}'.format(i) for i in range(1, N + 1)],\n",
        "                           index=df_all.columns)\n",
        "fprs, tprs, scores = [], [], []\n",
        "\n",
        "skf = StratifiedKFold(n_splits=N, random_state=N, shuffle=True)\n",
        "\n",
        "for fold, (trn_idx, val_idx) in enumerate(skf.split(X_train, y_train), 1):\n",
        "    print('Fold {}\\n'.format(fold))\n",
        "\n",
        "    # Fitting the model\n",
        "    leaderboard_model.fit(X_train[trn_idx], y_train[trn_idx])\n",
        "\n",
        "    # Computing Train AUC score\n",
        "    trn_fpr, trn_tpr, trn_thresholds = roc_curve(y_train[trn_idx],\n",
        "                                                 leaderboard_model.predict_proba(X_train[trn_idx])[:, 1])\n",
        "    trn_auc_score = auc(trn_fpr, trn_tpr)\n",
        "    # Computing Validation AUC score\n",
        "    val_fpr, val_tpr, val_thresholds = roc_curve(y_train[val_idx],\n",
        "                                                 leaderboard_model.predict_proba(X_train[val_idx])[:, 1])\n",
        "    val_auc_score = auc(val_fpr, val_tpr)\n",
        "\n",
        "    scores.append((trn_auc_score, val_auc_score))\n",
        "    fprs.append(val_fpr)\n",
        "    tprs.append(val_tpr)\n",
        "\n",
        "    # X_test probabilities\n",
        "    probs.loc[:, 'Fold_{}_Prob_0'.format(fold)] = leaderboard_model.predict_proba(X_test)[:, 0]\n",
        "    probs.loc[:, 'Fold_{}_Prob_1'.format(fold)] = leaderboard_model.predict_proba(X_test)[:, 1]\n",
        "    importances.iloc[:, fold - 1] = leaderboard_model.feature_importances_\n",
        "\n",
        "    oob += leaderboard_model.oob_score_ / N\n",
        "    print('Fold {} OOB Score: {}\\n'.format(fold, leaderboard_model.oob_score_))\n",
        "\n",
        "print('Average OOB Score: {}'.format(oob))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4ypHGDJAbZf",
        "outputId": "09ead231-194c-4f4d-d874-35483a891fd1"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (891, 26)\n",
            "y_train shape: (891,)\n",
            "X_test shape: (418, 26)\n",
            "Fold 1\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:    1.1s\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:    2.3s\n",
            "[Parallel(n_jobs=-1)]: Done 796 tasks      | elapsed:    3.7s\n",
            "[Parallel(n_jobs=-1)]: Done 1246 tasks      | elapsed:    4.9s\n",
            "[Parallel(n_jobs=-1)]: Done 1750 out of 1750 | elapsed:    6.3s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Done 796 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done 1246 tasks      | elapsed:    0.5s\n",
            "[Parallel(n_jobs=2)]: Done 1750 out of 1750 | elapsed:    0.7s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 796 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Done 1246 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=2)]: Done 1750 out of 1750 | elapsed:    0.5s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Done 796 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done 1246 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=2)]: Done 1750 out of 1750 | elapsed:    0.6s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 796 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Done 1246 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=2)]: Done 1750 out of 1750 | elapsed:    0.6s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 OOB Score: 0.8553370786516854\n",
            "\n",
            "Fold 2\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:    0.9s\n",
            "[Parallel(n_jobs=-1)]: Done 796 tasks      | elapsed:    1.5s\n",
            "[Parallel(n_jobs=-1)]: Done 1246 tasks      | elapsed:    2.4s\n",
            "[Parallel(n_jobs=-1)]: Done 1750 out of 1750 | elapsed:    3.3s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 796 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done 1246 tasks      | elapsed:    0.5s\n",
            "[Parallel(n_jobs=2)]: Done 1750 out of 1750 | elapsed:    0.7s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 796 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Done 1246 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done 1750 out of 1750 | elapsed:    0.5s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Done 796 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done 1246 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=2)]: Done 1750 out of 1750 | elapsed:    0.6s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Done 796 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done 1246 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=2)]: Done 1750 out of 1750 | elapsed:    0.6s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 OOB Score: 0.844319775596073\n",
            "\n",
            "Fold 3\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:    0.9s\n",
            "[Parallel(n_jobs=-1)]: Done 796 tasks      | elapsed:    1.6s\n",
            "[Parallel(n_jobs=-1)]: Done 1246 tasks      | elapsed:    2.5s\n",
            "[Parallel(n_jobs=-1)]: Done 1750 out of 1750 | elapsed:    3.5s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Done 796 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done 1246 tasks      | elapsed:    0.5s\n",
            "[Parallel(n_jobs=2)]: Done 1750 out of 1750 | elapsed:    0.7s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 796 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Done 1246 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=2)]: Done 1750 out of 1750 | elapsed:    0.5s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 796 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done 1246 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=2)]: Done 1750 out of 1750 | elapsed:    0.6s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Done 796 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done 1246 tasks      | elapsed:    0.5s\n",
            "[Parallel(n_jobs=2)]: Done 1750 out of 1750 | elapsed:    0.7s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 OOB Score: 0.8513323983169705\n",
            "\n",
            "Fold 4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:    0.9s\n",
            "[Parallel(n_jobs=-1)]: Done 796 tasks      | elapsed:    1.9s\n",
            "[Parallel(n_jobs=-1)]: Done 1246 tasks      | elapsed:    2.9s\n",
            "[Parallel(n_jobs=-1)]: Done 1750 out of 1750 | elapsed:    4.3s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Done 796 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done 1246 tasks      | elapsed:    0.5s\n",
            "[Parallel(n_jobs=2)]: Done 1750 out of 1750 | elapsed:    0.7s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 796 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Done 1246 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=2)]: Done 1750 out of 1750 | elapsed:    0.5s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Done 796 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done 1246 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=2)]: Done 1750 out of 1750 | elapsed:    0.6s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Done 796 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done 1246 tasks      | elapsed:    0.5s\n",
            "[Parallel(n_jobs=2)]: Done 1750 out of 1750 | elapsed:    0.7s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 OOB Score: 0.8359046283309958\n",
            "\n",
            "Fold 5\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:    0.9s\n",
            "[Parallel(n_jobs=-1)]: Done 796 tasks      | elapsed:    1.5s\n",
            "[Parallel(n_jobs=-1)]: Done 1246 tasks      | elapsed:    2.4s\n",
            "[Parallel(n_jobs=-1)]: Done 1750 out of 1750 | elapsed:    3.4s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 796 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done 1246 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=2)]: Done 1750 out of 1750 | elapsed:    0.6s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 796 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Done 1246 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=2)]: Done 1750 out of 1750 | elapsed:    0.5s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 796 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done 1246 tasks      | elapsed:    0.5s\n",
            "[Parallel(n_jobs=2)]: Done 1750 out of 1750 | elapsed:    0.6s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Done 796 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done 1246 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=2)]: Done 1750 out of 1750 | elapsed:    0.6s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 OOB Score: 0.8260869565217391\n",
            "\n",
            "Average OOB Score: 0.8425961674834928\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################################\n",
        "class_survived = [col for col in probs.columns if col.endswith('Prob_1')]\n",
        "probs['1'] = probs[class_survived].sum(axis=1) / N\n",
        "probs['0'] = probs.drop(columns=class_survived).sum(axis=1) / N\n",
        "probs['pred'] = 0\n",
        "pos = probs[probs['1'] >= 0.5].index\n",
        "probs.loc[pos, 'pred'] = 1\n",
        "\n",
        "y_pred = probs['pred'].astype(int)\n",
        "\n",
        "submission_df = pd.DataFrame(columns=['PassengerId', 'Survived'])\n",
        "submission_df['PassengerId'] = df_test['PassengerId']\n",
        "submission_df['Survived'] = y_pred.values\n",
        "submission_df.to_csv('submission.csv', header=True, index=False)\n",
        "\n",
        "output = pd.read_csv('submission.csv')\n",
        "print('\\n Correlation with ideal submission:', output['Survived'].corr(result_df['Survived']))\n",
        "result_df['percent'] = result_df['Survived'] == output['Survived']\n",
        "print('percent: \\n', (result_df['percent'].value_counts('True')))\n",
        "print('Real score on submission: 0.815789')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InM3e0zcCnl2",
        "outputId": "4a722e0b-1d22-43f7-f18e-189c79ea136d"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Correlation with ideal submission: 0.8184097744256326\n",
            "percent: \n",
            " True     0.916268\n",
            "False    0.083732\n",
            "Name: percent, dtype: float64\n",
            "Real score on submission: 0.815789\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E2cubRuVDANq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}